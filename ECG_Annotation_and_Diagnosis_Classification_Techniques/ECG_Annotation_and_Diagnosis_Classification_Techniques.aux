\relax 
\immediate\closeout\minitoc
\let \MiniTOC =N
\citation{clifford2006advanced}
\citation{mar2011optimization}
\@writefile{toc}{\contentsline {title}{ECG Annotation and Diagnosis Classification Techniques}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Yan Yan, Yige Wu, Xinbing Qin, Lei Wang}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Background}{1}}
\citation{clifford2006web}
\citation{silva2011dsp}
\@writefile{toc}{\contentsline {section}{\numberline {2}Technology Roadmap}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The technology roadmap of an ECG classification task.}}{2}}
\newlabel{fig:1}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}ECG Acquisition}{2}}
\citation{ai1996studies}
\citation{blanco2008ecg}
\citation{clifford2006advanced}
\citation{andreao2006ecg}
\citation{vullings1998automated}
\citation{vullings1997ecg}
\citation{sayadi2009model}
\citation{andreao2006ecg}
\citation{zigel2000weighted}
\citation{de2004automatic}
\citation{gacek2003genetic}
\citation{mark1982annotated,moody1990bih}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}ECG Signal Preprocessing}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}ECG Feature Extraction and Classification}{3}}
\citation{osowski2001ecg}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {3}Supervised learning Methods in ECG classification}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Supervised learning methods for ECG annotations}}{4}}
\newlabel{tab:0}{{1}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Unsupervised learning Methods in ECG classification}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An overview of the basic steps constituting a clustering process}}{4}}
\newlabel{fig:2}{{2}{4}}
\citation{erhan}
\citation{collobert}
\@writefile{toc}{\contentsline {section}{\numberline {5}Deep Learning in ECG Classification: A Preliminary Study Based on Deep Sparse Autoencoder}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Deep Neural Networks}{5}}
\citation{hinton2006reducing}
\citation{duda2012pattern}
\citation{bengio2007greedy}
\citation{bengio2009learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A typical neural network structure.}}{6}}
\newlabel{fig:3}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Autoencoders and Sparsity}{6}}
\citation{duda2012pattern,bishop2006pattern}
\citation{zou2012deep}
\citation{ufldl}
\citation{bengio2009learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Representation Learning}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A typical autoencoder neural network structure training process.}}{8}}
\newlabel{fig:4}{{4}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Fine-tuning and Classifier}{9}}
\citation{ngiam2011optimization}
\citation{de2004automatic}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Experiments and Results}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Datasets Preparation}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Samples after Segementation}}{11}}
\newlabel{tab:1}{{2}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Samples Dataset Settings}}{12}}
\newlabel{tab:2}{{3}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Classification Workflow}{12}}
\citation{melgan}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3}Classifier Performance Assessment}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.4}Results}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Test Result for 2-Hidden-Layer Autoencoder Network}}{13}}
\newlabel{tab:3}{{4}{13}}
\citation{mar}
\citation{chaza}
\citation{melgan}
\citation{jiang}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Test Results for 3-Hidden-Layer Autoencoder Network}}{14}}
\newlabel{tab:4}{{5}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Test Results for 4-Hidden-Layer Autoencoder Network}}{14}}
\newlabel{tab:5}{{6}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5}Comparison with Other Work}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Comparisons with Other Work Using Deep Autoencoder}}{14}}
\newlabel{tab:6}{{7}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Deep Learning in ECG Classification: A Two-lead ECG Classification Based on Deep Belief Network}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}The Deep Belief Network and Classifier}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}The Restricted Boltzmann Machine}{15}}
\citation{Leon}
\citation{Hinton02}
\citation{Peng}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Classifier and the Training of Multi-layer RBM}{16}}
\citation{Juergen,Bengio2009}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The RBMs are stacked to from a deep belief network(DBN). The RBM can be trained layer by layer. It is easy to construct a DBN with the trained RBMs. Also, a softmax model to fine-tune all parameters behind the last layer.}}{17}}
\newlabel{fig:5}{{5}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Combined optimization algorithm for multi-lead classifiers}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Experiment and Results}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Preprocessing and Segmentation}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Training and Fine-tuning}{19}}
\citation{Andreao}
\citation{Asl}
\citation{Melgani}
\citation{Sung}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Experiment Results}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Test result of three hidden Layers deep belief network using the first lead}}{20}}
\newlabel{tab:7}{{8}{20}}
\citation{Tadejko}
\citation{Banerjee}
\citation{Can}
\citation{Osowski}
\citation{Osowski}
\citation{Can}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Test result of three hidden Layers deep belief network using the first lead}}{21}}
\newlabel{tab:8}{{9}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Test result of three hidden Layers deep belief network using the first lead}}{21}}
\newlabel{tab:9}{{10}{21}}
\bibstyle{spbasic}
\bibdata{reference}
\bibcite{ai1996studies}{{1}{1996}{{Ai et~al}}{{Ai, Cui, Tang, Zhu, Ning, and Yang}}}
\bibcite{andreao2006ecg}{{2}{2006}{{Andre{\~a}o et~al}}{{Andre{\~a}o, Dorizzi, and Boudy}}}
\bibcite{bengio2009learning}{{3}{2009}{{Bengio}}{{}}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Comparisons with others' works}}{22}}
\newlabel{tab:10}{{11}{22}}
\@writefile{toc}{\contentsline {section}{Appendix}{22}}
\@writefile{toc}{\contentsline {section}{References}{22}}
\@mtwritefile{\contentsline {mtchap}{References}{22}}
\bibcite{bengio2007greedy}{{4}{2007}{{Bengio et~al}}{{Bengio, Lamblin, Popovici, Larochelle et~al}}}
\bibcite{bishop2006pattern}{{5}{2006}{{Bishop et~al}}{{}}}
\bibcite{blanco2008ecg}{{6}{2008}{{Blanco-Velasco et~al}}{{Blanco-Velasco, Weng, and Barner}}}
\bibcite{clifford2006web}{{7}{2006{a}}{{Clifford et~al}}{{Clifford, Azuaje, and McScharry}}}
\bibcite{clifford2006advanced}{{8}{2006{b}}{{Clifford et~al}}{{Clifford, Azuaje, McSharry et~al}}}
\bibcite{collobert}{{9}{2008}{{Collobert and Weston}}{{}}}
\bibcite{de2004automatic}{{10}{2004}{{De~Chazal et~al}}{{De~Chazal, O'Dwyer, and Reilly}}}
\bibcite{duda2012pattern}{{11}{2012}{{Duda et~al}}{{Duda, Hart, and Stork}}}
\bibcite{erhan}{{12}{2009}{{Erhan et~al}}{{Erhan, Manzagol, Bengio, Bengio, and Vincent}}}
\bibcite{gacek2003genetic}{{13}{2003}{{Gacek and Pedrycz}}{{}}}
\bibcite{hinton2006reducing}{{14}{2006}{{Hinton and Salakhutdinov}}{{}}}
\bibcite{mar2011optimization}{{15}{2011}{{Mar et~al}}{{Mar, Zaunseder, Martinez, Llamedo, and Poll}}}
\bibcite{mark1982annotated}{{16}{1982}{{Mark et~al}}{{Mark, Schluter, Moody, Devlin, and Chernoff}}}
\bibcite{moody1990bih}{{17}{1990}{{Moody and Mark}}{{}}}
\bibcite{ufldl}{{18}{2010}{{Ng et~al}}{{Ng, Ngiam, Foo, Mai, and Suen}}}
\bibcite{ngiam2011optimization}{{19}{2011}{{Ngiam et~al}}{{Ngiam, Coates, Lahiri, Prochnow, Le, and Ng}}}
\bibcite{osowski2001ecg}{{20}{2001}{{Osowski and Linh}}{{}}}
\bibcite{sayadi2009model}{{21}{2009}{{Sayadi and Shamsollahi}}{{}}}
\bibcite{silva2011dsp}{{22}{2011}{{Silva et~al}}{{Silva, Philominraj, and del R{\'\i }o}}}
\bibcite{vullings1997ecg}{{23}{1997}{{Vullings et~al}}{{Vullings, Verhaegen, and Verbruggen}}}
\bibcite{vullings1998automated}{{24}{1998}{{Vullings et~al}}{{Vullings, Verhaegen, and Verbruggen}}}
\bibcite{zigel2000weighted}{{25}{2000}{{Zigel et~al}}{{Zigel, Cohen, and Katz}}}
\bibcite{zou2012deep}{{26}{2012}{{Zou et~al}}{{Zou, Zhu, Yu, and Ng}}}
\immediate\closeout\minitoc
