\relax 
\citation{mar}
\citation{chaza}
\citation{melgan}
\citation{jiang,olmez,lin,osowski}
\citation{osowski,kundu}
\citation{andreao,coast}
\citation{zhu}
\citation{melgan,kampoura,khandoker}
\citation{chia}
\citation{jekova,christove,can}
\citation{inan}
\citation{banerjee}
\citation{stam}
\citation{lager}
\citation{mar}
\citation{clifford}
\citation{clifford}
\citation{physionet}
\citation{lager,nishizawa,maier}
\citation{palreddy,risk}
\citation{tadejko}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\citation{chaza}
\citation{physionet}
\citation{erhan}
\citation{collobert}
\@writefile{toc}{\contentsline {section}{\numberline {II}Electrocardiography Datasets and Arrhythmia Classes}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Arrhythmia Classes}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}The Unlabelled Dataset}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}The Open Labelled Datasets}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Experimental Methodology}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Models}{2}}
\citation{bengio2009}
\citation{bengio2009}
\citation{bengio2009}
\citation{bourlard}
\citation{jap}
\citation{bengio2007}
\citation{bengio2009}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces AAMI Classes Mapped from MIT-BIH Arrhythmia \& Long-term Database Types}}{3}}
\newlabel{Table1}{{I}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An one-hidden-layer autoencoder: first preset the output of the autoencoder to be the same as itself, train the neural network, then use the learned weights of the connections to calculate the representations of the raw input, in some literatures the process could be encoder and decoder \cite  {bengio2009}.}}{3}}
\newlabel{figure1}{{1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}1}Deep Neural Networks}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}2}Autoencoders and Sparsity}{3}}
\citation{bengio2009}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}3}Feature Self-taught Learning}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}4}Training Stacked Autoencoders}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}5}Fine-tuning}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-A}6}Softmax Classifier}{4}}
\citation{bengio2009}
\citation{vincent}
\citation{afonso}
\citation{chaza}
\citation{hinton}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Setup and Workflow}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}1}ECG Filtering}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}2}Heartbeat Detection}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}3}Heartbeat Segementation}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Samples after Segementation}}{5}}
\newlabel{table2}{{II}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Samples Dataset Settings}}{5}}
\newlabel{table3}{{III}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}4}Pretraining}{5}}
\citation{hinton}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The autoencoder pretraining process. (a) of figure 2 illustrated the one-hidden-layer autoencoder encoding and decoding processes; (b) of figure 2 illustrated a deeper network of 2-hidden-layer autoencoder which used the weight $W_1$ and outputs of feature layer (the gray layer) learned in (a); (c) of figure 2 is a comparison of normorlized data of raw input sample and reconstructed sample.}}{6}}
\newlabel{figure2}{{2}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Structure Settings}}{6}}
\newlabel{Table 5}{{IV}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}5}Fine-tuning}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-B}6}Test and Classifier Performance Assessment}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The softmax classifier. In each layer a bias node was included which had not been illustrated in the figure. }}{7}}
\newlabel{figure3}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Real-Time Classification}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The softmax model generalizes logistic regression to classification. Individual discriminant standard biased from global}}{7}}
\newlabel{bias}{{4}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A block diagram in real time applications with a second optimization fine-tuning process}}{7}}
\newlabel{chart}{{5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}The Classification Test Results}{7}}
\citation{melgan}
\citation{mar}
\citation{chaza}
\citation{melgan}
\citation{jiang}
\citation{hinton}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Test Result for 2-Hidden-Layer Autoencoder Network}}{8}}
\newlabel{table6}{{V}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Test Result for 3-Hidden-Layer Autoencoder Network}}{8}}
\newlabel{table6}{{VI}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Test Result for 4-Hidden-Layer Autoencoder Network}}{8}}
\newlabel{table7}{{VII}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The comparison of 3 kinds of network structure.}}{8}}
\newlabel{figure4}{{6}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Comparisons with Other Work}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces Comparisons with Other Work}}{8}}
\newlabel{table9}{{VIII}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussions}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}The autoencoder feature expression ability and pre-training}{8}}
\bibstyle{IEEEtran}
\bibdata{bare_jrnl}
\bibcite{mar}{1}
\bibcite{chaza}{2}
\bibcite{melgan}{3}
\bibcite{jiang}{4}
\bibcite{olmez}{5}
\bibcite{lin}{6}
\bibcite{osowski}{7}
\bibcite{kundu}{8}
\bibcite{andreao}{9}
\bibcite{coast}{10}
\bibcite{zhu}{11}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Test results}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusions}{9}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  A: Notations}{9}}
\@writefile{toc}{\contentsline {section}{References}{9}}
\bibcite{kampoura}{12}
\bibcite{khandoker}{13}
\bibcite{chia}{14}
\bibcite{jekova}{15}
\bibcite{christove}{16}
\bibcite{can}{17}
\bibcite{inan}{18}
\bibcite{banerjee}{19}
\bibcite{stam}{20}
\bibcite{lager}{21}
\bibcite{clifford}{22}
\bibcite{physionet}{23}
\bibcite{nishizawa}{24}
\bibcite{maier}{25}
\bibcite{palreddy}{26}
\bibcite{risk}{27}
\bibcite{tadejko}{28}
\bibcite{erhan}{29}
\bibcite{collobert}{30}
\bibcite{bengio2009}{31}
\bibcite{bourlard}{32}
\bibcite{jap}{33}
\bibcite{bengio2007}{34}
\bibcite{vincent}{35}
\bibcite{afonso}{36}
\bibcite{hinton}{37}
\@writefile{toc}{\contentsline {section}{Biographies}{10}}
\@writefile{toc}{\contentsline {subsection}{Jan Doe}{10}}
\@writefile{toc}{\contentsline {subsection}{John Doe}{10}}
\@writefile{toc}{\contentsline {subsection}{Jane Doe}{10}}
